{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion and transformation notebook\n",
    "\n",
    "This notebook uses the data ingestion standard and run the transformations and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "main_time_counter = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions for feature engineering:\n",
    "\n",
    "def decode_stacked_json(stacked_json_string, pos=0, decoder=json.JSONDecoder()):\n",
    "    \"\"\"Yield multiple JSON objects and restart parsing from the previous position\n",
    "    Input must be of the form: {'key1': value1, 'key2': value2}{'key3': value3, 'key4': value4\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            json_object, pos = decoder.raw_decode(stacked_json_string, pos)\n",
    "        except json.JSONDecodeError:\n",
    "            break\n",
    "        yield json_object\n",
    "       \n",
    "        \n",
    "def after_wake_exercise(class_5min,bedtime_end):\n",
    "    \"\"\"returns the number of minutes with medium to high MET scores \n",
    "    within the first 3 hours of wake-time as a proxy for whether or not exercise \n",
    "    occurs after waking up\"\"\"\n",
    "    if isinstance(class_5min,float) or isinstance(bedtime_end, float):\n",
    "        return np.nan\n",
    "    else:\n",
    "        #convert the str integer into a list of integers\n",
    "        class_5min_list = list(map(int, class_5min))\n",
    "        #--take the timestamp from the datetime string, extracts hh:mm data, and converts to a number--#\n",
    "        wake_hr_min = int(''.join(bedtime_end.split('T')[1][0:5].split(':')))\n",
    "        #---calculate minutes lapsed since 4am and wake up time---#\n",
    "        #---rescale minutes into 5 minute intervals to find the number of elements at which to offset class_5min--#\n",
    "        offset = int(((wake_hr_min - 400)/100)*(60/5))\n",
    "\n",
    "        #subset observations between wake up and 3 hrs post wake up (24*5=120 min)\n",
    "        morning_obs = class_5min_list[offset:offset+36]\n",
    "\n",
    "        #tally total minutes spent in medium to high intensity exercise (3-4)\n",
    "        total_min = sum([1 for obs in morning_obs if obs >=3])*5\n",
    "        return total_min\n",
    "\n",
    "\n",
    "def before_sleep_exercise(class_5min,bedtime_start):\n",
    "    \"\"\"returns the number of minutes with medium to high MET scores \n",
    "    within the last 3 hours of wake-time as a proxy for whether or not exercise \n",
    "    occurs in the evening close to bedtime\"\"\"\n",
    "    if isinstance(class_5min,float) or isinstance(bedtime_start, float):\n",
    "        return np.nan\n",
    "    else:\n",
    "        #convert the str integer into a list of integers\n",
    "        class_5min_list = list(map(int, class_5min))\n",
    "        #--take the timestamp from the datetime string, extracts hh:mm data, and converts to a number--#\n",
    "        sleep_hr_min = int(''.join(bedtime_start.split('T')[1][0:5].split(':')))\n",
    "        #---calculate minutes lapsed since 4am and wake up time---#\n",
    "        #---rescale minutes into 5 minute intervals to find the number of elements at which to offset class_5min--#\n",
    "        offset = int(((sleep_hr_min - 400)/100)*(60/5))\n",
    "\n",
    "        #subset observations 3 hours before sleep time\n",
    "        evening_obs = class_5min_list[offset-36:offset]\n",
    "\n",
    "        #tally total minutes spent in medium to high intensity exercise (3-4)\n",
    "        total_min = sum([1 for obs in evening_obs if obs >=3])*5\n",
    "        return total_min\n",
    "\n",
    "\n",
    "def noon_exercise(class_5min):\n",
    "    \"\"\"returns the number of minutes with medium to high MET scores \n",
    "    between noon and two local time\"\"\"\n",
    "    if isinstance(class_5min,float):\n",
    "        return np.nan\n",
    "    else:\n",
    "        #convert the str integer into a list of integers\n",
    "        class_5min_list = list(map(int, class_5min))\n",
    "\n",
    "        #---calculate minutes lapsed since 4am and 12 (8*12 (5min intervasl in an hr)---#\n",
    "\n",
    "        offset = 96\n",
    "\n",
    "        #subset observations between noon and 2 pm\n",
    "        noon_obs = class_5min_list[offset:offset+24]\n",
    "\n",
    "        #tally total minutes spent in medium to high intensity exercise (3-4)\n",
    "        total_min = sum([1 for obs in noon_obs if obs >=3])*5\n",
    "        return total_min\n",
    " \n",
    "    \n",
    "def evening_exercise(class_5min):\n",
    "    \"\"\"returns the number of minutes with medium to high MET scores \n",
    "    between noon and two local time\"\"\"\n",
    "    if isinstance(class_5min, float):\n",
    "        return np.nan\n",
    "    else:\n",
    "        #convert the str integer into a list of integers\n",
    "        class_5min_list = list(map(int, class_5min))\n",
    "\n",
    "        #---calculate minutes lapsed since 4am and 5pm (13*12) --there are 12 5min intervasl in an hr---#\n",
    "\n",
    "        offset = 156\n",
    "\n",
    "        #subset observations between 5pm and 7 pm post wake up\n",
    "        eve_obs = class_5min_list[offset:offset+24]\n",
    "\n",
    "        #tally total minutes spent in medium to high intensity exercise (3-4)\n",
    "        total_min = sum([1 for obs in eve_obs if obs >=3])*5\n",
    "        return total_min\n",
    "\n",
    "\n",
    "def after_midnight(timestamp_str):\n",
    "    \"\"\"indicator for whether or not bedtime started after midnight (in early am hours)\"\"\"\n",
    "    try:\n",
    "        day, hour = timestamp_str.split('T')\n",
    "        hr = int(hour[0:2]) #extract hr from timestamp\n",
    "        if 0 <= hr < 6:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def age_bin(e):\n",
    "    \"\"\"Bins: 20s, 30s, 40s, and 50s +\"\"\"\n",
    "    if  20 <= e < 30:\n",
    "        return \"20s\"\n",
    "    elif 30 <= e < 40:\n",
    "        return \"30s\"\n",
    "    elif 40 <= e < 50:\n",
    "        return \"40s\"\n",
    "    elif e > 50:\n",
    "        return \"50s plus\"\n",
    "    else:\n",
    "        return np.nan\n",
    "   \n",
    "    \n",
    "def height_bin(e):\n",
    "    \"\"\"Bins: 0, 150, 160, 170, 180, 190. In centimeters \"\"\"\n",
    "    if  0 < e < 150:\n",
    "        return \"less than 150 cm\"\n",
    "    elif 150 <= e <= 160:\n",
    "        return \"150s\"\n",
    "    elif 160 <= e <= 170:\n",
    "        return \"160s\"\n",
    "    elif 170 <= e <= 180:\n",
    "        return \"170s\"\n",
    "    elif 180 <= e <= 190:\n",
    "        return \"180s\"\n",
    "    elif e > 190:\n",
    "        return \"greater than 190 cm\"\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "    \n",
    "def weight_bin(e):\n",
    "    \"\"\"Bins: 0, 65, 80, 95. In kilograms\"\"\"\n",
    "    if  0 < e <=65:\n",
    "        return \"less than 65 kg\"\n",
    "    elif 65 <= e <= 80:\n",
    "        return \"65 to 80 kg\"\n",
    "    elif 80 <= e <= 95:\n",
    "        return \"80 to 95 kg\"\n",
    "    elif e > 95:\n",
    "        return \"more than 95 kg\"\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def period_over_period(dataframe,usrid,date_col, metric_cols,days_offset):\n",
    "    \"\"\"for each datetime in the dataframe's date column, computes the metric column's\n",
    "    difference from the period days prior:\n",
    "    returns WoW differences with the corresponding userid and datetime as a tuple\"\"\"\n",
    "    #--first recorded observation date + days_offset\n",
    "    init_date = min(dataframe[date_col]) + pd.DateOffset(days_offset)\n",
    "    pop_list = []\n",
    "    for dt in dataframe[date_col]:\n",
    "        if dt < init_date: # won't have 7 days prior for the first 7 observations so flag as np.nan\n",
    "            pop_list.append([usrid,dt]+[np.nan for i in np.arange(len(metric_cols))])\n",
    "        else:\n",
    "            try: #---need to account for missing dates\n",
    "                pop_diff_list = []\n",
    "                for metric in metric_cols:\n",
    "                    #grab the value from the prior 7(or n) days -- need to index on 0 since values returns array of array\n",
    "                    prd_prior = dataframe[dataframe[date_col] == dt + pd.DateOffset(-days_offset)][metric].values[0]\n",
    "                    #grab the current value\n",
    "                    prd_current = dataframe[dataframe[date_col] == dt][metric].values[0]\n",
    "                    #take the diff - list of differences\n",
    "                    try:\n",
    "                        pop_diff = (prd_current/prd_prior)-1\n",
    "                    except ZeroDivisionError: #1 if prior period was 0 (basically capping at 100% increase)\n",
    "                        pop_diff = 1\n",
    "                   \n",
    "                    pop_diff_list.append(pop_diff)\n",
    "                    \n",
    "                pop_list.append([usrid,dt]+[i for i in pop_diff_list])\n",
    "            except:\n",
    "                #---if missing dates then use nan values\n",
    "                pop_list.append([usrid,dt]+[np.nan for i in np.arange(len(metric_cols))])\n",
    "\n",
    "    return pop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "full_path = 'data_ingestion/{}'.format(date.today().strftime('%d%m%Y'))\n",
    "os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "# create a simple \"log file\" on the log ingestion directory. Can be helpful to debug and later on for feature monitoring.\n",
    "log_file = open(full_path + '/log.txt', 'a+')\n",
    "log_file.write(time.asctime() + ' - Starting data ingestion process\\n')\n",
    "\n",
    "# if a data file exists, load it, otherwise, create the directory, download a new one and load the DFs:\n",
    "if not os.path.isfile(full_path + '/raw_dataset.zip'):    \n",
    "    start = time.time() # time the data ingestion from Memento\n",
    "    log_file.write(time.asctime() + ' - Day file not found. Starting data download\\n')\n",
    "    \n",
    "    # try: \n",
    "    ACCESS_TOKEN = ##ENVIRONMENT VARIABLE HERE##\n",
    "    headers = {'Authorization': 'Bearer ' + ACCESS_TOKEN }\n",
    "    URL = \"https://api.mementolabs.io/data/dataset\"\n",
    "    # changed this code to deal with streaming json from 10/20/2019 on. Must double check if this will be the final \n",
    "    # format\n",
    "    data = requests.get(URL, headers=headers).content.decode(encoding='utf-8')\n",
    "\n",
    "    # save the raw data to disk as zip it to allow re-processing without new download\n",
    "    zip_file = full_path + '/raw_dataset.zip'\n",
    "    zipObj = ZipFile(zip_file, 'w', compression=zipfile.ZIP_DEFLATED)\n",
    "    zipObj.writestr('raw_dataset.txt', data)\n",
    "    zipObj.close()\n",
    "\n",
    "    # print('total run time:',time.time() - start)\n",
    "    log_file.write(time.asctime() + ' - JSON File downloaded and saved. Time: ' + f'{(time.time() - start):.1f}' + ' secs.\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define intermediate data frames:\n",
    "users = pd.DataFrame()\n",
    "sleep = pd.DataFrame()\n",
    "readiness = pd.DataFrame()\n",
    "activity = pd.DataFrame()\n",
    "experiments = pd.DataFrame()\n",
    "experiments2 = pd.DataFrame()\n",
    "\n",
    "start = time.time()\n",
    "raw_data = str(zipfile.ZipFile(full_path + '/raw_dataset.zip').read('raw_dataset.txt'), encoding='utf-8')\n",
    "log_file.write(time.asctime() + ' - raw data file loaded. Time: ' + f'{(time.time() - start):.1f}' + ' secs.\\n')\n",
    "\n",
    "start = time.time()\n",
    "log_file.write(time.asctime() + ' - creating feature space\\n')\n",
    "\n",
    "active_user_counter = 0\n",
    "xp_counter = 0\n",
    "\n",
    "for user_json in decode_stacked_json(raw_data):\n",
    "    # in the stacked json file, each json object corresponds to all data of one user. So here I will perform all \n",
    "    # feature transformation that are user-dependent before I append the whole user data to a final data frame.\n",
    "    user_user = pd.DataFrame.from_records([user_json['userInfo']['userInfo']])\n",
    "    \n",
    "    try: # user signup date may be useful to define if a user model is relevant or not:\n",
    "        user_user['signupDate'] = pd.to_datetime(user_json['signupDate'])\n",
    "    except KeyError:\n",
    "        user_user['signupDate'] = np.nan\n",
    "\n",
    "    # one hot encode gender data:\n",
    "    try:\n",
    "        user_user['gender'] = np.where(user_user['gender'] == 'male', True, False)\n",
    "    except KeyError:\n",
    "        user_user['gender'] = np.nan\n",
    "\n",
    "    user_user.rename(columns={'gender':'is_male'}, inplace=True)\n",
    "\n",
    "    try:\n",
    "        user_user['age'] = user_user['age']\n",
    "    except KeyError:\n",
    "        user_user['age'] = np.nan\n",
    "\n",
    "    try:\n",
    "        user_user['height'] = user_user['height']\n",
    "    except KeyError:\n",
    "        user_user['height'] = np.nan\n",
    "\n",
    "    try:\n",
    "        user_user['weight'] = user_user['weight']\n",
    "    except KeyError:\n",
    "        user_user['weight'] = np.nan\n",
    "\n",
    "    # send to a final users data frame:\n",
    "    users = users.append(user_user)\n",
    "\n",
    "    # skip users without sleep records (strictly experiment users, or users of other hardware)\n",
    "    if len(user_json['sleep']) == 0: continue\n",
    "    active_user_counter += 1\n",
    "\n",
    "    # unpack data for each user in a specific data frame\n",
    "    user_sleep = pd.DataFrame.from_records([i for i in user_json['sleep']])\n",
    "    user_readiness = pd.DataFrame.from_records([i for i in user_json['readiness']])\n",
    "    user_activity = pd.DataFrame.from_records([i for i in user_json['activity']])\n",
    "\n",
    "    # each DF has a `score` feature. Renaming to avoid later confusion:\n",
    "    user_sleep.rename(columns = {'score':'sleep_score'}, inplace=True)\n",
    "    user_activity.rename(columns = {'score':'activity_score'}, inplace=True)\n",
    "    user_readiness.rename(columns = {'score':'readiness_score'}, inplace=True)\n",
    "\n",
    "    #### ---------- SLEEP FEATURES ----------- ####\n",
    "    # sleep features normalizing and transformations:\n",
    "    user_sleep['awake'] = user_sleep['awake']/user_sleep['duration']\n",
    "    user_sleep['deep'] = user_sleep['deep']/user_sleep['duration']\n",
    "    user_sleep['light'] = user_sleep['light']/user_sleep['duration']\n",
    "    user_sleep['onset_latency'] = user_sleep['onset_latency']/user_sleep['duration']\n",
    "    user_sleep['rem'] = user_sleep['rem']/user_sleep['duration']\n",
    "    user_sleep['restless'] = user_sleep['light']/100\n",
    "\n",
    "    # bins for sleep score to Shiraz's models:\n",
    "    bins = [0, 75, 85, 100]\n",
    "    names = ['fair', 'good', 'greate']\n",
    "    user_sleep['good_sleep'] = pd.cut(user_sleep['sleep_score'], bins=bins, labels=names)\n",
    "\n",
    "    # creating new sleep features:\n",
    "    user_sleep['summary_date'] = pd.to_datetime(user_sleep['summary_date'])\n",
    "    user_sleep['user_id'] = user_user['user_id'][0]\n",
    "    user_sleep['user_date'] = user_sleep['user_id'] + '|' + user_sleep['summary_date'].astype('str')\n",
    "\n",
    "    user_sleep.set_index('summary_date', inplace = True, drop=False)\n",
    "\n",
    "    # build D - 1 and D - 2 scores:\n",
    "    user_sleep['sleep_score_D-1'] = user_sleep['sleep_score'].shift()[user_sleep.index.shift(1,freq='1D')]\n",
    "    user_sleep['sleep_score_D-2'] = user_sleep['sleep_score'].shift()[user_sleep.index.shift(1,freq='2D')]\n",
    "    user_sleep['deep_D-1'] = user_sleep['deep'].shift()[user_sleep.index.shift(1,freq='1D')]\n",
    "    user_sleep['deep_D-2'] = user_sleep['deep'].shift()[user_sleep.index.shift(1,freq='2D')]\n",
    "    user_sleep['rem_D-1'] = user_sleep['rem'].shift()[user_sleep.index.shift(1,freq='1D')]\n",
    "    user_sleep['rem_D-2'] = user_sleep['rem'].shift()[user_sleep.index.shift(1,freq='2D')]\n",
    "\n",
    "    # build 7, 14, and 21 rolling average scores:\n",
    "    user_sleep['rol_sleep_score_7d'] = pd.DataFrame.rolling(user_sleep['sleep_score'].shift(1, freq='1D'),\n",
    "                                                      window=7, min_periods=3).mean()\n",
    "    # shifted 7 day score rolled feature. This is target feature to predict your average 7 days ahead\n",
    "    user_sleep['avg_sleep_score_next_week'] = user_sleep['rol_sleep_score_7d'].shift(-1,freq='7D')\n",
    "\n",
    "    user_sleep['rol_sleep_score_14d'] = pd.DataFrame.rolling(user_sleep['sleep_score'].shift(1, freq='1D'),\n",
    "                                                      window=14, min_periods=10).mean()\n",
    "    user_sleep['rol_sleep_score_21d'] = pd.DataFrame.rolling(user_sleep['sleep_score'].shift(1, freq='1D'),\n",
    "                                                      window=21, min_periods=17).mean()\n",
    "\n",
    "    # build other 7-day rolling features for peter's models later:\n",
    "    user_sleep['rol_bedtime_end_delta_7d'] = pd.DataFrame.rolling(user_sleep['bedtime_end_delta'].shift(1, freq='1D'),\n",
    "                                                            window=7, min_periods=3).mean()\n",
    "    user_sleep['rol_bedtime_start_delta_7d'] = pd.DataFrame.rolling(user_sleep['bedtime_start_delta'].shift(1, freq='1D'),\n",
    "                                                                    window=7, min_periods=3).mean()\n",
    "    user_sleep['rol_onset_latency_7d'] = pd.DataFrame.rolling(user_sleep['onset_latency'].shift(1, freq='1D'),\n",
    "                                                              window=7, min_periods=3).mean()\n",
    "    user_sleep['rol_duration_7d'] = pd.DataFrame.rolling(user_sleep['duration'].shift(1, freq='1D'),\n",
    "                                                         window=7, min_periods=3).mean()\n",
    "\n",
    "    # build is_traveling\n",
    "    user_sleep['is_traveling'] = np.where(user_sleep['timezone'] == \n",
    "                                                    user_sleep['timezone'].value_counts().idxmax(), False, True)\n",
    "    \n",
    "    user_sleep['tz_delta'] = (user_sleep['timezone'] - user_sleep['timezone'].value_counts().idxmax()) / (-60)\n",
    "    \n",
    "    # build rol_bedtime_start_21d\n",
    "    user_sleep['rol_bedtime_start_21d'] = pd.DataFrame.rolling(user_sleep['bedtime_start_delta'].shift(1, freq='1D'),\n",
    "                                                                 window=21, min_periods=10).mean()\n",
    "\n",
    "    # build avg_bedtime_start_delta, create dummy variables for deviation (-3, -2, -1, 1, 2, 3)\n",
    "    user_sleep['rol_bedtime_start_std_21d'] = pd.DataFrame.rolling(user_sleep['bedtime_start_delta'].shift(1, freq='1D'),\n",
    "                                                                 window=21, min_periods=10).std()\n",
    "\n",
    "    user_sleep['bedtime_start_dev'] = (user_sleep['bedtime_start_delta'] - user_sleep['rol_bedtime_start_21d'])/\\\n",
    "                                      user_sleep['rol_bedtime_start_std_21d']\n",
    "\n",
    "    user_sleep['bedtime_start_dev'] = np.where(user_sleep['bedtime_start_dev'] >= 0, \n",
    "                                                        np.ceil(user_sleep['bedtime_start_dev']),\n",
    "                                                        np.floor(user_sleep['bedtime_start_dev']))\n",
    "    # cap standard deviations to -3 or +3:\n",
    "    user_sleep['bedtime_start_dev'] = np.where(user_sleep['bedtime_start_dev'] <= -3,-3, user_sleep['bedtime_start_dev'])\n",
    "\n",
    "    user_sleep['bedtime_start_dev'] = np.where(user_sleep['bedtime_start_dev'] >= 3,3,\n",
    "                                                        user_sleep['bedtime_start_dev'])\n",
    "\n",
    "    user_sleep = pd.get_dummies(user_sleep, columns=['bedtime_start_dev'])\n",
    "\n",
    "    user_sleep.rename(columns = { 'bedtime_start_dev_-3.0':'bedtime_start_dev-3', \n",
    "                                  'bedtime_start_dev_-2.0':'bedtime_start_dev-2', \n",
    "                                  'bedtime_start_dev_-1.0': 'bedtime_start_dev-1',\n",
    "                                  'bedtime_start_dev_1.0': 'bedtime_start_dev+1',\n",
    "                                  'bedtime_start_dev_2.0': 'bedtime_start_dev+2',\n",
    "                                  'bedtime_start_dev_3.0': 'bedtime_start_dev+3'},\n",
    "                                  inplace = True)\n",
    "\n",
    "    user_sleep.drop(labels = ['rol_bedtime_start_21d','rol_bedtime_start_std_21d'], axis = 1, inplace = True)\n",
    "\n",
    "    # build avg_bedtime_end_delta, create dummy variables for deviation. (-3, -2, -1, 1, 2, 3)\n",
    "    user_sleep['rol_bedtime_end_21d'] = pd.DataFrame.rolling(user_sleep['bedtime_end_delta'].shift(1, freq='1D'),\n",
    "                                                                 window=21, min_periods=10).mean()\n",
    "\n",
    "    user_sleep['rol_bedtime_end_std_21d'] = pd.DataFrame.rolling(user_sleep['bedtime_end_delta'].shift(1, freq='1D'),\n",
    "                                                                 window=21, min_periods=10).std()\n",
    "\n",
    "    user_sleep['bedtime_end_dev'] = (user_sleep['bedtime_end_delta'] - user_sleep['rol_bedtime_end_21d'])/\\\n",
    "                                    user_sleep['rol_bedtime_end_std_21d']\n",
    "\n",
    "    user_sleep['bedtime_end_dev'] = np.where(user_sleep['bedtime_end_dev'] >= 0, \n",
    "                                             np.ceil(user_sleep['bedtime_end_dev']),\n",
    "                                             np.floor(user_sleep['bedtime_end_dev']))\n",
    "    # cap standard deviations to -3 or +3:\n",
    "    user_sleep['bedtime_end_dev'] = np.where(user_sleep['bedtime_end_dev'] <= -3,-3,\n",
    "                                             user_sleep['bedtime_end_dev'])\n",
    "\n",
    "    user_sleep['bedtime_end_dev'] = np.where(user_sleep['bedtime_end_dev'] >= 3,3,\n",
    "                                             user_sleep['bedtime_end_dev'])\n",
    "\n",
    "    user_sleep = pd.get_dummies(user_sleep, columns=['bedtime_end_dev'])\n",
    "\n",
    "    user_sleep.rename(columns = {'bedtime_end_dev_-3.0':'bedtime_end_dev-3', \n",
    "                                 'bedtime_end_dev_-2.0':'bedtime_end_dev-2', \n",
    "                                 'bedtime_end_dev_-1.0': 'bedtime_end_dev-1',\n",
    "                                 'bedtime_end_dev_1.0': 'bedtime_end_dev+1',\n",
    "                                 'bedtime_end_dev_2.0': 'bedtime_end_dev+2',\n",
    "                                 'bedtime_end_dev_3.0': 'bedtime_end_dev+3'},\n",
    "                                 inplace = True)\n",
    "\n",
    "    user_sleep.drop(labels = ['rol_bedtime_end_21d','rol_bedtime_end_std_21d'], axis = 1, inplace = True)\n",
    "\n",
    "    # build avg_duration, create dummy variables for deviation (-3, -2, -1, 1, 2, 3)\n",
    "    user_sleep['rol_duration_21d'] = pd.DataFrame.rolling(user_sleep['duration'].shift(1, freq='1D'),\n",
    "                                                                 window=21, min_periods=10).mean()\n",
    "\n",
    "    user_sleep['rol_duration_std_21d'] = pd.DataFrame.rolling(user_sleep['duration'].shift(1, freq='1D'),\n",
    "                                                                 window=21, min_periods=10).std()\n",
    "\n",
    "    user_sleep['duration_dev'] = (user_sleep['duration'] - user_sleep['rol_duration_21d'])/\\\n",
    "                                 user_sleep['rol_duration_std_21d']\n",
    "\n",
    "    user_sleep['duration_dev'] = np.where(user_sleep['duration_dev'] >= 0, \n",
    "                                          np.ceil(user_sleep['duration_dev']),\n",
    "                                          np.floor(user_sleep['duration_dev']))\n",
    "\n",
    "    # cap standard deviations to -3 or +3:\n",
    "    user_sleep['duration_dev'] = np.where(user_sleep['duration_dev'] <= -3,-3, user_sleep['duration_dev'])\n",
    "\n",
    "    user_sleep['duration_dev'] = np.where(user_sleep['duration_dev'] >= 3,3, user_sleep['duration_dev'])\n",
    "\n",
    "    user_sleep = pd.get_dummies(user_sleep, columns=['duration_dev'])\n",
    "\n",
    "    user_sleep.rename(columns = {'duration_dev_-3.0':'duration_dev-3', \n",
    "                              'duration_dev_-2.0':'duration_dev-2', \n",
    "                              'duration_dev_-1.0':'duration_dev-1',\n",
    "                              'duration_dev_1.0': 'duration_dev+1',\n",
    "                              'duration_dev_2.0': 'duration_dev+2',\n",
    "                              'duration_dev_3.0': 'duration_dev+3'},\n",
    "                              inplace = True)\n",
    "\n",
    "    user_sleep.drop(labels = ['rol_duration_21d','rol_duration_std_21d'], axis = 1, inplace = True)\n",
    "\n",
    "    # build dummy to day of the week.\n",
    "    user_sleep['weekday'] = user_sleep['summary_date'].dt.weekday\n",
    "\n",
    "    # build is_workday\n",
    "    user_sleep['is_workday'] = np.where(user_sleep['weekday'] < 5, True, False)\n",
    "\n",
    "    # one-hot encode weekdays\n",
    "    user_sleep = pd.get_dummies(user_sleep, columns=['weekday'])\n",
    "\n",
    "    user_sleep.rename(columns = {'weekday_0':'weekday_mon', \n",
    "                              'weekday_1':'weekday_tue', \n",
    "                              'weekday_2': 'weekday_wed',\n",
    "                              'weekday_3': 'weekday_thu',\n",
    "                              'weekday_4': 'weekday_fri',\n",
    "                              'weekday_5': 'weekday_sat',\n",
    "                              'weekday_6': 'weekday_sun'}, \n",
    "                              inplace = True)\n",
    "\n",
    "    user_sleep.set_index('user_date', inplace = True)\n",
    "\n",
    "\n",
    "    sleep = sleep.append(user_sleep)\n",
    "\n",
    "\n",
    "    #### ---------- ACTIVITY FEATURES ----------- ####\n",
    "    # Creating the new activity related features. Because several activity features depends on user info and/or \n",
    "    # sleep features, I will add the features needed here:\n",
    "    user_activity['user_date'] = user_user['user_id'][0] + '|' + user_activity['summary_date'].astype('str')\n",
    "\n",
    "    user_activity['age'] = user_user['age'][0]\n",
    "    user_activity['height'] = user_user['height'][0]\n",
    "    user_activity['weight'] = user_user['weight'][0]\n",
    "    user_activity['is_male'] = user_user['is_male'][0]\n",
    "\n",
    "    user_activity['summary_date'] = pd.to_datetime(user_activity['summary_date'])   \n",
    "    user_activity = user_activity.merge(user_sleep[['bedtime_start', 'bedtime_end', 'summary_date']], \n",
    "                                        on='summary_date', how='left')\n",
    "\n",
    "    user_activity.set_index('summary_date', inplace = True, drop=False)\n",
    "\n",
    "    user_activity['met_min_medium_plus'] = user_activity['met_min_medium'] + \\\n",
    "                                                    user_activity['met_min_high']\n",
    "\n",
    "    user_activity['age_bin'] = user_activity['age'].apply(age_bin)\n",
    "    user_activity['height_bin'] = user_activity['height'].apply(height_bin)\n",
    "    user_activity['weight_bin'] = user_activity['weight'].apply(weight_bin)\n",
    "    user_activity['sleep_afterMidnight'] = user_activity['bedtime_start'].apply(after_midnight)\n",
    "    user_activity['afterwake_exercise_min'] = [after_wake_exercise(c,b) for c,b in \\\n",
    "                                               zip(user_activity['class_5min'],\n",
    "                                                   user_activity['bedtime_end'])]\n",
    "    user_activity['beforesleep_exercise_min'] = [before_sleep_exercise(c,b) for c,b in \\\n",
    "                                                 zip(user_activity['class_5min'],\n",
    "                                                     user_activity['bedtime_start'])]\n",
    "    user_activity['noon_exercise_min'] = user_activity['class_5min'].apply(noon_exercise)\n",
    "    user_activity['eve_exercise_min'] = user_activity['class_5min'].apply(evening_exercise)\n",
    "\n",
    "    # create activity variables that will be rolled within a 7 day window\n",
    "    activity_vars_to_roll = ['cal_total', 'high', 'medium',\n",
    "                             'steps','inactive', 'non_wear', 'activity_score',\n",
    "                             'met_min_medium', 'met_min_high', 'met_min_medium_plus',\n",
    "                             'score_move_every_hour', 'score_stay_active', \n",
    "                             'beforesleep_exercise_min', 'afterwake_exercise_min',\n",
    "                             'noon_exercise_min', 'eve_exercise_min']\n",
    "\n",
    "    for feature in activity_vars_to_roll:\n",
    "        user_activity['rol_' + feature + '_7d'] = pd.DataFrame.rolling(user_activity[feature].shift(1, freq='1D'),\n",
    "                                                      window=7, min_periods=3).mean()\n",
    "\n",
    "    user_activity.set_index('user_date', inplace = True)\n",
    "    activity = activity.append(user_activity)\n",
    "\n",
    "\n",
    "    #### ---------- READINESS FEATURES ----------- ####\n",
    "    # As we are not doing any transformation in the readiness data frame, append directly to the final one:\n",
    "    user_readiness['user_date'] = user_user['user_id'][0] + '|' + user_readiness['summary_date'].astype('str')\n",
    "    user_readiness.set_index('user_date', inplace=True)\n",
    "    readiness = readiness.append(user_readiness)\n",
    "\n",
    "    #### ---------- EXPERIMENTS FEATURES ----------- ####\n",
    "    # Not all users had experiments done:\n",
    "    if len(user_json['previousExperiments']) != 0:\n",
    "    # extract and flattens only the features needed: date, treatment, effect, and if if is a baseline date.\n",
    "    # NOTE: In Memento one experiment have one treatment, but can have multiple effects, however we are not interested\n",
    "    #       in the intermediate effect, but on the effect on final sleep score. Therefore, we will ignore the effects\n",
    "    #       the users set and incorporate the experiments to the main data frame.\n",
    "      \n",
    "        user_experiments = pd.DataFrame.from_records([i for i in user_json['previousExperiments']])\n",
    "        xp_counter += len(user_json['previousExperiments'])\n",
    "    \n",
    "        try:\n",
    "            treatment_randomized = pd.io.json.json_normalize(user_experiments.design[0]).isRandomized[0]\n",
    "        except:\n",
    "            treatment_randomized = 0\n",
    "        \n",
    "        # parse experiment types:\n",
    "        experiment_type = user_experiments.experimentName[0].split('(')[0]\n",
    "        \n",
    "        # unroll checkIns object\n",
    "        checkIns = pd.DataFrame()\n",
    "        checkIns = pd.io.json.json_normalize(user_experiments.checkIns[0])\n",
    "        \n",
    "        # add experiment randomization, type, and user_ids to checkins\n",
    "        checkIns['treatment_randomized'] = treatment_randomized\n",
    "        checkIns['experiment_type'] = experiment_type\n",
    "        checkIns['user_id'] = user_user['user_id'][0]\n",
    "        \n",
    "        # filter out only the necessary columns (helps the join later)\n",
    "        checkIns = checkIns.filter(['date','user_id','treatment_randomized','experiment_type','compliance'])\n",
    "    \n",
    "        checkIns['user_date'] = checkIns['user_id'] + '|' + checkIns['date'].astype('str')\n",
    "        checkIns.set_index('user_date', inplace=True)\n",
    "    \n",
    "        experiments = experiments.append(checkIns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log the time and main descriptive statistics of the final datasets for data governance purposes\n",
    "log_file.write(time.asctime() + ' - Feature space created. Time: ' + f'{(time.time() - start):.1f}' + ' secs.\\n')\n",
    "log_file.write(time.asctime() + '     - Total active users: ' + str(active_user_counter) + '\\n')\n",
    "log_file.write(time.asctime() + '     - ACTIVITY data frame shape: ' + str(activity.shape) + '\\n')\n",
    "log_file.write(time.asctime() + '     - SLEEP data frame shape: ' + str(sleep.shape) + '\\n')\n",
    "log_file.write(time.asctime() + '     - READINESS data frame shape: ' + str(readiness.shape) + '\\n')\n",
    "log_file.write(time.asctime() + '     - EXPERIMENTS data frame shape: ' + str(experiments.shape) + '\\n')\n",
    "log_file.write(time.asctime() + '       - finished EXPERIMENTS count: ' + str(xp_counter) + '\\n')\n",
    "log_file.write(time.asctime() + '       - finished EXPERIMENTS user-days: ' + str(experiments.shape[0]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Save each DF as a csv file (case needed to work on a single file in teh future\n",
    "users_file_path = full_path + '/users_data.csv'\n",
    "sleep_file_path = full_path + '/sleep_data.csv'\n",
    "readiness_file_path = full_path + '/readiness_data.csv'\n",
    "activity_file_path = full_path + '/activity_data.csv'\n",
    "df_gold_file_path = full_path + '/gold.csv'\n",
    "experiments_file_path = full_path + '/experiments.csv'\n",
    "\n",
    "users.to_csv(users_file_path, index=None, encoding='utf-8')\n",
    "sleep.to_csv(sleep_file_path, index_label='user_date', encoding='utf-8')\n",
    "readiness.to_csv(readiness_file_path, index_label='user_Date', encoding='utf-8')\n",
    "activity.to_csv(activity_file_path, index_label='user_Date', encoding='utf-8')\n",
    "experiments.to_csv(experiments_file_path, index=None, encoding='utf-8')\n",
    "\n",
    "# Drop duplicate columns and merge all dfs in a big \"gold\" df. Will keep only one of the summary dates (readiness)\n",
    "readiness.drop(['period_id'], axis=1, inplace=True)\n",
    "sleep.drop(['summary_date', 'timezone','period_id'], axis=1, inplace=True)\n",
    "activity.drop(['bedtime_start', 'bedtime_end', 'summary_date', 'timezone', 'total'], axis=1, inplace=True)\n",
    "experiments.drop(['date', 'user_id'], axis=1, inplace=True)\n",
    "\n",
    "# merge all data frames into one. NOTE: sleep, activity and readiness are inner joins, so only days with all records are\n",
    "# used. For experiments we use outer join, so user_days without experiments will have NaN on those features.\n",
    "df_gold = sleep.merge(activity, left_index=True, right_index=True)\n",
    "df_gold = df_gold.merge(readiness, left_index=True, right_index=True)\n",
    "df_gold = df_gold.merge(experiments, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "df_gold.to_csv(df_gold_file_path, index_label='user_date', encoding='utf-8')\n",
    "\n",
    "log_file.write(time.asctime() + '     - GOLD data frame shape: ' + str(df_gold.shape) + '\\n')\n",
    "\n",
    "# zip all files in a gold_dataset.zip\n",
    "files = [users_file_path, sleep_file_path, readiness_file_path,\n",
    "         activity_file_path, df_gold_file_path, experiments_file_path]\n",
    "\n",
    "files_names = ['users_data.csv', 'sleep_data.csv', 'readiness_data.csv',\n",
    "         'activity_data.csv', 'gold.csv', 'experiments.csv']\n",
    "\n",
    "zip_file = full_path + '/gold_dataset.zip'\n",
    "zipObj = zipfile.ZipFile(zip_file, mode='w')\n",
    "\n",
    "for i in range(len(files)):\n",
    "    zipObj.write(files[i], arcname=files_names[i])    \n",
    "\n",
    "zipObj.close()\n",
    "\n",
    "# delete unzipped CSVs\n",
    "for i in range(len(files)):\n",
    "    os.remove(files[i])\n",
    "    \n",
    "log_file.write(time.asctime() + ' - All data frames saved and zipped. Time: ' + f'{(time.time() - start):.1f}' + ' secs.\\n')\n",
    "\n",
    "log_file.write(time.asctime() + ' - Total time spent in the routine: ' + f'{(time.time() - main_time_counter):.1f}' + ' secs.\\n')\n",
    "log_file.write(time.asctime() + ' ##### ----- DATA INGESTION ROUTINE FINALIZED ----- #####\\n\\n')\n",
    "\n",
    "log_file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
